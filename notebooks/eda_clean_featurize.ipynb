{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-10 23:16:50.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mData loaded with shape: (3333, 11)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('../data/bronze/telecom_churn.csv')\n",
    "logger.info(f\"Data loaded with shape: {df_raw.shape}\")\n",
    "# logger.info(f\"Columns: {df_raw.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the metadata from a json file:\n",
    "with open('../data/bronze/telecom_churn_metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "col_desc_list = metadata['recordSet'][0]['field']\n",
    "# get name and description for eac of the element in the list\n",
    "col_desc_dict = {el['name']: el['description'] for el in col_desc_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'average daytime minutes per month'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use the dict to see the description of the columns\n",
    "col_desc_dict['DayMins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the columns:\n",
    "\n",
    "label_col = 'Churn'\n",
    "columns = df_raw.columns.tolist()\n",
    "columns.remove(label_col)\n",
    "columns.append(label_col)\n",
    "df_raw = df_raw[columns]\n",
    "\n",
    "# only retain columns with no missing values\n",
    "retained_cols = df_raw.columns[df_raw.isnull().mean() == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train_raw, df_inference_raw = train_test_split(df_raw, test_size=0.2, random_state=99, stratify=df_raw['Churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the df_raw_inference\n",
    "df_inference_raw.to_csv('../data/inference/df_inference_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning and Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are to be reproduced if requests from raw data:\n",
    "- Remove cols that were removed from the initial steps\n",
    "- Scaled numberical values by the saved scaler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with missing values:\n",
    "df_train_cleaned = df_train_raw[retained_cols]\n",
    "feat_cols = df_train_cleaned.columns.tolist()\n",
    "feat_cols.remove(label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2666, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize the numberical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid data leakage, we need to split the data before any preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df_train_cleaned, test_size=0.2, random_state=99, stratify=df_train_cleaned[label_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_featurized = df_train.copy(deep=True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# note we only fit the scaler on feature cols, not the label\n",
    "scaler = StandardScaler()\n",
    "df_train_featurized[feat_cols] = scaler.fit_transform(df_train_featurized[feat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only transform the test set without fitting to avoid data leakage\n",
    "df_test_featurized = df_test.copy(deep=True)\n",
    "df_test_featurized[feat_cols] = scaler.transform(df_test_featurized[feat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export it to Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_featurized.to_parquet('../data/gold/telcom_churn_train_featurized.parquet', index=False)\n",
    "df_test_featurized.to_parquet('../data/gold/telcom_churn_test_featurized.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Persisting the artifacts from data cleaning and featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are important for the service later, which will apply the same logic to the raw input data (from df_raw_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../services/models/scaler.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# save the columns_without_missing to a file\n",
    "with open('../services/models/retained_cols.json', 'w') as f:\n",
    "    json.dump(retained_cols.tolist(), f)\n",
    "\n",
    "# save the feature columns to a file\n",
    "with open('../services/models/feat_cols.json', 'w') as f:\n",
    "    json.dump(feat_cols, f)\n",
    "\n",
    "joblib.dump(scaler, '../services/models/scaler.pkl')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telco-churn-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
